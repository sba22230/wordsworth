{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb44b1a7",
   "metadata": {},
   "source": [
    "Name: wordsworth\n",
    "Description: Frequency analysis tool\n",
    "Author: autonomoid\n",
    "Date: 2014-06-22\n",
    "Licence: GPLv3\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program. If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d4ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cee4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font effects --> fancy console colours in bash\n",
    "underline = \"\\x1b[1;4m\"\n",
    "black = \"\\x1b[1;30m\"\n",
    "red = \"\\x1b[1;31m\"\n",
    "green = \"\\x1b[1;32m\"\n",
    "yellow = \"\\x1b[1;33m\"\n",
    "blue = \"\\x1b[1;34m\"\n",
    "purple = \"\\x1b[1;35m\"\n",
    "turquoise = \"\\x1b[1;36m\"\n",
    "normal = \"\\x1b[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226e7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word = ''\n",
    "previous_pair = ''\n",
    "previous_triple = ''\n",
    "previous_quad = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd72013",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stats = {\n",
    "              'total_chars': 0,\n",
    "              'total_words': 0,\n",
    "              'total_sentences': 0,\n",
    "              'max_length': 0,\n",
    "              'min_length': 999,\n",
    "              'mean_length': -1,\n",
    "              'longest_word': '',\n",
    "              'shortest_word': '',\n",
    "              'char_counts': {\n",
    "                              'a': 0.0, 'b': 0.0, 'c': 0.0, 'd': 0.0, 'e': 0.0, 'f': 0.0,\n",
    "                              'g': 0.0, 'h': 0.0, 'i': 0.0, 'j': 0.0, 'k': 0.0, 'l': 0.0,\n",
    "                              'm': 0.0, 'n': 0.0, 'o': 0.0, 'p': 0.0, 'q': 0.0, 'r': 0.0,\n",
    "                              's': 0.0, 't': 0.0, 'u': 0.0, 'v': 0.0, 'w': 0.0, 'x': 0.0,\n",
    "                              'y': 0.0, 'z': 0.0\n",
    "                             },\n",
    "              'char_percentages': {\n",
    "                                   'a': 0.0, 'b': 0.0, 'c': 0.0, 'd': 0.0, 'e': 0.0, 'f': 0.0,\n",
    "                                   'g': 0.0, 'h': 0.0, 'i': 0.0, 'j': 0.0, 'k': 0.0, 'l': 0.0,\n",
    "                                   'm': 0.0, 'n': 0.0, 'o': 0.0, 'p': 0.0, 'q': 0.0, 'r': 0.0,\n",
    "                                   's': 0.0, 't': 0.0, 'u': 0.0, 'v': 0.0, 'w': 0.0, 'x': 0.0,\n",
    "                                   'y': 0.0, 'z': 0.0\n",
    "                                  },\n",
    "              'lexical_density': -1,\n",
    "              'ARI_score': -1\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92615926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_word_frequencies(n_word_counter, top_n, output_file, tag=None):\n",
    "    total_entries = sum(n_word_counter.values())\n",
    "    unique_entries = len(n_word_counter)\n",
    "    if total_entries > 0:\n",
    "        m = n_word_counter.most_common(min(unique_entries, top_n))\n",
    "        n = len(m[0][0].split(' '))\n",
    "\n",
    "        if tag == None:\n",
    "            print ('\\n===' + blue + ' Commonest ' + str(n) + '-words' + normal + ' ===')\n",
    "            out.write('\\n=== Commonest ' + str(n) + '-words ===\\n')\n",
    "        else:\n",
    "            print ('\\n===' + blue + ' Commonest ' + tag + normal + ' ===')\n",
    "            out.write('\\n=== Commonest ' + tag + ' ===\\n')\n",
    "\n",
    "        for i in range(0, min(unique_entries, top_n)):\n",
    "            n_word = m[i][0]\n",
    "            count = m[i][1]\n",
    "            perc = 100.0 * (count / float(total_entries))\n",
    "\n",
    "            print (str(i + 1) + ' = ' + purple + n_word +\n",
    "                   normal + ' (' + purple + str(count).split('.')[0] + normal +\n",
    "                   ' = ' + purple + str(perc)[:5] + '%' + normal + ')')\n",
    "\n",
    "            output_file.write(str(i + 1) + ' = ' + n_word + ' (' + str(count).split('.')[0] +\n",
    "            ' = ' + str(perc)[:5] + '%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a2f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(word_stats, output_file):\n",
    "    print ('\\n===' + blue + ' RESULTS ' + normal + '===')\n",
    "    out.write('=== RESULTS ===\\n')\n",
    "\n",
    "    print ('File = ' + purple + str(args.inputfile) + normal)\n",
    "    out.write('File = ' + str(args.inputfile) + '\\n')\n",
    "\n",
    "    print ('Longest word = ' + purple + str(word_stats['longest_word']) + normal +\n",
    "           ' (' + purple + str(word_stats['max_length']) + normal + ')')\n",
    "\n",
    "    out.write('Longest word = ' + str(word_stats['longest_word']) +\n",
    "           ' (' + str(word_stats['max_length']) + ')\\n')\n",
    "\n",
    "    print ('Shortest word = ' + purple + str(word_stats['shortest_word']) + normal +\n",
    "           ' (' + purple + str(word_stats['min_length']) + normal + ')')\n",
    "\n",
    "    out.write('Shortest word = ' + str(word_stats['shortest_word']) +\n",
    "           ' (' + str(word_stats['min_length']) + ')\\n')\n",
    "\n",
    "    print ('Mean word length /chars = ' + purple + str(word_stats['mean_length']) +\n",
    "            normal)\n",
    "\n",
    "    out.write('Mean word length /chars = ' + str(word_stats['mean_length']) + '\\n')\n",
    "\n",
    "    print ('Total words parsed = ' + purple +\n",
    "            str(word_stats['total_words']).split('.')[0] + normal)\n",
    "\n",
    "    out.write('Total words parsed = ' +\n",
    "            str(word_stats['total_words']).split('.')[0] + '\\n')\n",
    "\n",
    "    print ('Total chars parsed = ' + purple + str(word_stats['total_chars']) +\n",
    "            normal)\n",
    "\n",
    "    out.write('Total chars parsed = ' + str(word_stats['total_chars']) + '\\n')\n",
    "\n",
    "    for i in range(max_n_word):\n",
    "        print_n_word_frequencies(counters[i], args.top_n, out)\n",
    "\n",
    "    print_n_word_frequencies(personal_pronoun_counter, args.top_n, out, tag=\"Personal Pronouns\")\n",
    "    print_n_word_frequencies(noun_counter, args.top_n, out, tag=\"Nouns\")\n",
    "    print_n_word_frequencies(adjective_counter, args.top_n, out, tag=\"Adjectives\")\n",
    "    print_n_word_frequencies(adverb_counter, args.top_n, out, tag=\"Adverbs\")\n",
    "    print_n_word_frequencies(verb_counter, args.top_n, out, tag=\"Verbs\")\n",
    "\n",
    "    total_dev = 0.0\n",
    "\n",
    "    print ('\\n===' + blue + ' FREQUENCY ANALYSIS ' + normal + '===')\n",
    "    out.write('\\n=== FREQUENCY ANALYSIS ===\\n')\n",
    "\n",
    "    # Display information about character frequencies.\n",
    "    #for char in sorted(word_stats['char_percentages'].iterkeys()):\n",
    "    for char  in sorted(word_stats['char_percentages'].keys()):\n",
    "        bar = ''\n",
    "        perc = word_stats['char_percentages'][char]\n",
    "\n",
    "        # Percentage deviation from random distribution of characters.\n",
    "        dev = 100.0 * (abs((100.0 / 26.0) - perc) / (100.0 / 26.0))\n",
    "        total_dev += dev\n",
    "\n",
    "        for i in range(0, int(perc)):\n",
    "            bar += '#'\n",
    "\n",
    "        print (char + ' |' + red + bar + normal + ' ' + str(perc)[:4] +\n",
    "                '% (' + str(dev)[:4] + '% deviation from random)')\n",
    "\n",
    "        out.write(char + ' |' + bar + ' ' + str(perc)[:4] + '% (' +\n",
    "                str(dev)[:4] + '% deviation from random)\\n')\n",
    "\n",
    "    print ('\\nTotal percentage deviation from random = ' +\n",
    "            str(total_dev).split('.')[0] + '%')\n",
    "\n",
    "    out.write('\\nTotal percentage deviation from random = ' +\n",
    "            str(total_dev).split('.')[0] + '%')\n",
    "\n",
    "    average_dev = total_dev / 26.0\n",
    "\n",
    "    print ('Average percentage deviation from random = ' +\n",
    "            str(average_dev)[:4] + '%')\n",
    "\n",
    "    out.write('\\nAverage percentage deviation from random = ' +\n",
    "              str(average_dev)[:4] + '%')\n",
    "\n",
    "    print ('\\n===' + blue + ' WORD LENGTH ' + normal + '===')\n",
    "    out.write('\\n\\n=== WORD LENGTH ===\\n')\n",
    "\n",
    "    # Display data above word length frequency.\n",
    "    length_counts = word_length_counter.most_common()\n",
    "    for length in length_counts:\n",
    "        l = length[0]\n",
    "        perc = 100.0 * length[1] / float(word_stats['total_words'])\n",
    "        bar = ''\n",
    "        for i in range(0, int(perc)):\n",
    "            bar += '#'\n",
    "\n",
    "        print (l + ' |' + red + bar + normal + ' ' + str(perc)[:4] +\n",
    "                '% (' + str(length[1]) + ')')\n",
    "\n",
    "        out.write(l + ' |' + bar + ' ' + str(perc)[:4] +\n",
    "                '% (' + str(length[1]) + ')\\n')\n",
    "\n",
    "    print ('\\nLexical density = ' + str(word_stats['lexical_density'])[:5] + '%')\n",
    "\n",
    "    out.write('\\nLexical density = ' + str(word_stats['lexical_density'])[:5] + '%')\n",
    "\n",
    "    print ('ARI (Automated Readability Index) score = ' + str(word_stats['ARI_score'])[:5])\n",
    "\n",
    "    out.write('\\nARI (Automated Readability Index) score = ' + str(word_stats['ARI_score'])[:5] + '%')\n",
    "\n",
    "    print ('\\nWritten results to ' + args.inputfile.split('.')[0] + '-stats.txt\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to update the parsing of arguements to allow for file and other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3a21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --filename INPUTFILE [--ntuple MAX_N_WORD]\n",
      "                             [--top TOP_N] [--allow-digits ALLOWDIGITS]\n",
      "                             [--ignore IGNORE_LIST]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"4f0ac56e-7ceb-4cd1-9616-a5d1db38db0a\" --shell=9002 --transport=\"tcp\" --iopub=9004\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Perform letter, word and n-tuple frequency analysis on text files.')\n",
    "    parser.add_argument('--filename', '-f', dest='inputfile', required=True, help='Text file to parse.')\n",
    "    parser.add_argument('--ntuple', '-n', dest='max_n_word', required=False, default=4, type=int, help='The maximum length n-tuple of words. Default is 4.')\n",
    "    parser.add_argument('--top', '-t', dest='top_n', required=False, default=20, type=int, help='List the top t most frequent n-words. Default is 20.')\n",
    "    parser.add_argument('--allow-digits', '-d', dest='allowdigits', default=False, required=False, help='Allow digits to be parsed (true/false). Default is false.')\n",
    "    parser.add_argument('--ignore', '-i', dest='ignore_list', required=False, help='Comma-delimted list of things to ignore')\n",
    " \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ignore_list = str(args.ignore_list).split(\",\")\n",
    "\n",
    "    # Dynamically allocated n-word counters\n",
    "    max_n_word = args.max_n_word\n",
    "    n_words = ['' for i in range(max_n_word)]\n",
    "    prev_n_words = ['' for i in range(max_n_word)]\n",
    "    counters = [collections.Counter() for i in range(max_n_word)]\n",
    "\n",
    "    # Word length counter\n",
    "    word_length_counter = collections.Counter()\n",
    "\n",
    "    # Read in all of the words in a file\n",
    "    print (\"[+] Reading text from '\" + args.inputfile + \"'...\")\n",
    "    text = open(args.inputfile).read().lower()\n",
    "\n",
    "    # Use nltk to classify/tag each word/token.\n",
    "    print (\"[+] Tokenizing text...\")\n",
    "    text = open(args.inputfile).read().lower()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    print (\"[+] Tagging tokens...\")\n",
    "    tagger = nltk.UnigramTagger(nltk.corpus.brown.tagged_sents())\n",
    "    tagged_tokens = tagger.tag(tokens)\n",
    "\n",
    "    print (\"[+] Tallying tags...\")\n",
    "    personal_pronoun_counter = collections.Counter()\n",
    "    adjective_counter = collections.Counter()\n",
    "    adverb_counter = collections.Counter()\n",
    "    noun_counter = collections.Counter()\n",
    "    verb_counter = collections.Counter()\n",
    "\n",
    "    for token in tagged_tokens:\n",
    "\n",
    "        if token[1] == None:\n",
    "            continue\n",
    "\n",
    "        elif 'PPS' in token[1]:\n",
    "            personal_pronoun_counter[token[0]] += 1\n",
    "\n",
    "        elif 'JJ' in token[1]:\n",
    "            adjective_counter[token[0]] += 1\n",
    "\n",
    "        elif 'NN' in token[1]:\n",
    "            noun_counter[token[0]] += 1\n",
    "\n",
    "        elif 'RB' in token[1]:\n",
    "            adverb_counter[token[0]] += 1\n",
    "\n",
    "        elif 'VB' in token[1]:\n",
    "            verb_counter[token[0]] += 1\n",
    "\n",
    "    # Shall we include digits?\n",
    "    if args.allowdigits:\n",
    "        words = re.findall(r\"['\\-\\w]+\", text)\n",
    "    else:\n",
    "        words = re.findall(r\"['\\-A-Za-z]+\", text)\n",
    "\n",
    "    print (\"[+] Counting sentences...\")\n",
    "    #word_stats['total_sentences'] = len(nltk.sent_tokenize(text.decode('utf-8')))\n",
    "    word_stats['total_sentences'] = len(nltk.sent_tokenize(text))\n",
    "\n",
    "    print (\"[+] Performing frequency analysis of n-words...\")\n",
    "    for word in words:\n",
    "    \n",
    "        if word in ignore_list:\n",
    "            continue\n",
    "        \n",
    "        word = word.strip(r\"&^%$#@!\")\n",
    "\n",
    "        # Allow hyphenated words, but not hyphens as words on their own.\n",
    "        if word == '-':\n",
    "            continue\n",
    "\n",
    "        # Record all word lengths\n",
    "        length = len(word)\n",
    "        word_length_counter[str(length)] += 1\n",
    "\n",
    "        # Record longest word length\n",
    "        if length > word_stats['max_length']:\n",
    "            word_stats['max_length'] = length\n",
    "            word_stats['longest_word'] = word\n",
    "\n",
    "        # Record shortest word length\n",
    "        if length < word_stats['min_length']:\n",
    "            word_stats['min_length'] = length\n",
    "            word_stats['shortest_word'] = word\n",
    "\n",
    "        # Keep track of the total number of words and chars read.\n",
    "        word_stats['total_chars'] += length\n",
    "        word_stats['total_words'] += 1.0\n",
    "\n",
    "        # Tally the charaters in each word.\n",
    "        for char in word:\n",
    "            if char.lower() in word_stats['char_counts']:\n",
    "                word_stats['char_counts'][char.lower()] += 1.0\n",
    "\n",
    "        # Tally words.\n",
    "        for i in range(1, max_n_word):\n",
    "            if prev_n_words[i - 1] != '':\n",
    "                n_words[i] = prev_n_words[i - 1] + ' ' + word\n",
    "                counters[i][n_words[i]] += 1\n",
    "\n",
    "        n_words[0] = word\n",
    "        counters[0][word] += 1\n",
    "\n",
    "        for i in range(0, max_n_word):\n",
    "            prev_n_words[i] = n_words[i]\n",
    "\n",
    "    # Calculate the mean word length\n",
    "    word_stats['mean_length'] = word_stats['total_chars'] / word_stats['total_words']\n",
    "\n",
    "    # Calculate relative character frequencies\n",
    "    for char in word_stats['char_counts']:\n",
    "        char_count = word_stats['char_counts'][char]\n",
    "        total_chars = word_stats['total_chars']\n",
    "        percentage = 100.0 * (char_count / total_chars)\n",
    "        word_stats['char_percentages'][char] = percentage\n",
    "\n",
    "    # Calculate the lexical density of the text.\n",
    "    total_unique_words = len(counters[0])\n",
    "    total_words = sum(counters[0].values())\n",
    "    word_stats['lexical_density'] = 100.0 * total_unique_words / float(total_words)\n",
    "\n",
    "    # Calculate the ARI score.\n",
    "    # See http://www.usingenglish.com/members/text-analysis/help/readability.html\n",
    "    total_words = sum(counters[0].values())\n",
    "    ASL = total_words / float(word_stats['total_sentences'])\n",
    "    ALW = word_stats['total_chars'] / float(total_words)\n",
    "    word_stats['ARI_score'] = (0.5 * ASL) + (4.71 * ALW) - 21.43\n",
    "\n",
    "    # Print results\n",
    "    out = open(args.inputfile.split('.')[0] + '-stats.txt', 'w')\n",
    "    print_results(word_stats, out)\n",
    "    out.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
