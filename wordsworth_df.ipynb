{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b07101",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1502a8d",
   "metadata": {},
   "source": [
    "Name: wordsworth\n",
    "Description: Frequency analysis tool\n",
    "Author: autonomoid\n",
    "Date: 2014-06-22\n",
    "Licence: GPLv3\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program. If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2353dc70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy  # used to calculate clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f466f4e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class wordsworth:\n",
    "    args = 0\n",
    "    ignore_list = []\n",
    "    out = 0\n",
    "    words = []\n",
    "    previous_word = ''\n",
    "    previous_pair = ''\n",
    "    previous_triple = ''\n",
    "    previous_quad = ''\n",
    "    max_n_word = 4  \n",
    "    n_words = []\n",
    "    prev_n_words = []\n",
    "    counters = []\n",
    "\n",
    "    word_stats = {\n",
    "                  'file_name': '',\n",
    "                  'total_chars': 0,\n",
    "                  'total_words': 0,\n",
    "                  'total_sentences': 0,\n",
    "                  'word_max_length': 0,\n",
    "                  'word_min_length': 999,\n",
    "                  'mean_length': -1,\n",
    "                  'longest_word': '',\n",
    "                  'shortest_word': '',\n",
    "                  'sentence_max_length': 0, \n",
    "                  'max_tree_depth': 0,                  \n",
    "                  'longest_sentence': '',\n",
    "                  'mtd_sentence': '',\n",
    "\n",
    "                #   'char_counts': {\n",
    "                #                   'a': 0.0, 'b': 0.0, 'c': 0.0, 'd': 0.0, 'e': 0.0, 'f': 0.0,\n",
    "                #                   'g': 0.0, 'h': 0.0, 'i': 0.0, 'j': 0.0, 'k': 0.0, 'l': 0.0,\n",
    "                #                   'm': 0.0, 'n': 0.0, 'o': 0.0, 'p': 0.0, 'q': 0.0, 'r': 0.0,\n",
    "                #                   's': 0.0, 't': 0.0, 'u': 0.0, 'v': 0.0, 'w': 0.0, 'x': 0.0,\n",
    "                #                   'y': 0.0, 'z': 0.0\n",
    "                #                  },\n",
    "                #   'char_percentages': {\n",
    "                #                        'a': 0.0, 'b': 0.0, 'c': 0.0, 'd': 0.0, 'e': 0.0, 'f': 0.0,\n",
    "                #                        'g': 0.0, 'h': 0.0, 'i': 0.0, 'j': 0.0, 'k': 0.0, 'l': 0.0,\n",
    "                #                        'm': 0.0, 'n': 0.0, 'o': 0.0, 'p': 0.0, 'q': 0.0, 'r': 0.0,\n",
    "                #                        's': 0.0, 't': 0.0, 'u': 0.0, 'v': 0.0, 'w': 0.0, 'x': 0.0,\n",
    "                #                        'y': 0.0, 'z': 0.0\n",
    "                #                       },\n",
    "                  'lexical_density': -1,\n",
    "                  'num_clauses': -1,\n",
    "                  'Haliday_density': -1\n",
    "                  }\n",
    "\n",
    "\n",
    "    def __init__(self, commandline_args):\n",
    "        args = commandline_args\n",
    "        self.ignore_list = str(args.ignore_list).split(\",\")\n",
    "        \n",
    "\n",
    "    def print_results(self):\n",
    "    # Create a DataFrame from the word_stats dictionary\n",
    "        df = pd.DataFrame(self.word_stats, index=[0])\n",
    "        return df\n",
    "      \n",
    "        \n",
    "\n",
    "    def init_word_counters(self):\n",
    "        self.max_n_word = args.max_n_word\n",
    "        self.n_words = ['' for i in range(self.max_n_word)]\n",
    "        self.prev_n_words = ['' for i in range(self.max_n_word)]\n",
    "        self.counters = [collections.Counter() for i in range(self.max_n_word)]\n",
    "\n",
    "\n",
    "    def read_file(self):\n",
    "        print(\"[+] Analysing '\" + args.inputfile + \"'\")\n",
    "        self.word_stats['file_name'] = os.path.basename(args.inputfile)\n",
    "        if args.allow_digits:\n",
    "            self.words = re.findall(r\"['\\-\\w]+\", open(args.inputfile, 'r', encoding='utf-8').read().lower())\n",
    "        else:\n",
    "            self.words = re.findall(r\"['\\-A-Za-z]+\", open(args.inputfile, 'r', encoding='utf-8').read().lower())\n",
    "        self.doc = open(args.inputfile, 'r', encoding='utf-8').read()\n",
    "    \n",
    "\n",
    "    \n",
    "    def compute_stats(self):\n",
    "        for word in self.words:\n",
    "        \n",
    "            if word in self.ignore_list:\n",
    "                continue\n",
    "        \n",
    "            word = word.strip(r\"&^%$#@!\")\n",
    "\n",
    "            # Allow hyphenated words, but not hyphens as words on their own.\n",
    "            if word == '-':\n",
    "                continue\n",
    "\n",
    "            length = len(word)\n",
    "\n",
    "            # Record longest word length\n",
    "            if length > self.word_stats['word_max_length']:\n",
    "                self.word_stats['word_max_length'] = length\n",
    "                self.word_stats['longest_word'] = word\n",
    "\n",
    "            # Record shortest word length\n",
    "            if length < self.word_stats['word_min_length']:\n",
    "                self.word_stats['word_min_length'] = length\n",
    "                self.word_stats['shortest_word'] = word\n",
    "\n",
    "            # Keep track of the total number of words and chars read.\n",
    "            self.word_stats['total_chars'] += length\n",
    "            self.word_stats['total_words'] += 1.0\n",
    "\n",
    "            # # Note the charaters in each word.\n",
    "            # for char in word:\n",
    "            #     if char.lower() in self.word_stats['char_counts']:\n",
    "            #         self.word_stats['char_counts'][char.lower()] += 1.0\n",
    "\n",
    "            # Tally words.\n",
    "            for i in range(1, self.max_n_word):\n",
    "                if self.prev_n_words[i - 1] != '':\n",
    "                    self.n_words[i] = self.prev_n_words[i - 1] + ' ' + word\n",
    "                    self.counters[i][self.n_words[i]] += 1\n",
    "\n",
    "            self.n_words[0] = word\n",
    "            self.counters[0][word] += 1\n",
    "\n",
    "            for i in range(0, self.max_n_word):\n",
    "                self.prev_n_words[i] = self.n_words[i]\n",
    "\n",
    "        # Calculate the mean word length\n",
    "        self.word_stats['mean_length'] = self.word_stats['total_chars'] / self.word_stats['total_words']\n",
    "\n",
    "        # # Calculate relative character frequencies\n",
    "        # for char in self.word_stats['char_counts']:\n",
    "        #     char_count = self.word_stats['char_counts'][char]\n",
    "        #     total_chars = self.word_stats['total_chars']\n",
    "        #     percentage = 100.0 * (char_count / total_chars)\n",
    "        #     self.word_stats['char_percentages'][char] = percentage\n",
    "\n",
    "        # Calculate the lexical density of the text.\n",
    "        total_unique_words = len(self.counters[0])\n",
    "        total_words = sum(self.counters[0].values())\n",
    "        self.word_stats['lexical_density'] = 100.0 * total_unique_words / float(total_words)\n",
    "        \n",
    "        # calculate the Lexical Items to Number of Clauses  - Haliday Lexical density\n",
    "        \n",
    "        # Function to compute the tree depth of a sentence - a measure of Structural Complexity \n",
    "        def tree_depth(sentence):\n",
    "            \"\"\"\n",
    "            Calculate the depth of the tree parsed by spaCy.\n",
    "            \"\"\"\n",
    "            def _depth(token):\n",
    "                if not list(token.children):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 1 + max(_depth(child) for child in token.children)\n",
    "            return max(_depth(token) for token in sentence)\n",
    "        \n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        doc = nlp(self.doc)\n",
    "       \n",
    "        try: \n",
    "            num_clauses = sum(1 for sent in doc.sents for token in sent if token.dep_ == 'ROOT' and token.pos_ == 'VERB')\n",
    "        except ValueError:\n",
    "            num_clauses = -1\n",
    "        self.word_stats['total_sentences'] = len(list(doc.sents))\n",
    "        self.word_stats['num_clauses'] = num_clauses\n",
    "        self.word_stats['Haliday_density'] = 100.0 * total_unique_words / float(num_clauses)\n",
    "        longest_sentence = max(doc.sents, key=lambda sentence: len(sentence))\n",
    "        self.word_stats['longest_sentence'] = longest_sentence.text\n",
    "        self.word_stats['sentence_max_length'] = len(longest_sentence)\n",
    "        #shortest_sentence = min(doc.sents, key=lambda sentence: len(sentence))\n",
    "        #self.word_stats['sentence_min_length'] = len(shortest_sentence)\n",
    "        #self.word_stats['shortest_sentence'] = shortest_sentence.text\n",
    "        max_depth = 0 \n",
    "        sentence_mtd = ''\n",
    "        for sent in doc.sents:\n",
    "            # Calculate the depth of the current sentence\n",
    "            depth = tree_depth(sent)\n",
    "            # Update the maximum depth if necessary\n",
    "            if depth > max_depth:\n",
    "                max_depth = depth\n",
    "                sentence_mtd = sent.text\n",
    "\n",
    "        self.word_stats['max_tree_depth'] = max_depth\n",
    "        self.word_stats['mtd_sentence'] = sentence_mtd                  \n",
    "                  \n",
    "                  \n",
    "\n",
    "        return(self.word_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e221987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Analysing 'C:/Source/Repos/Research-Proposal-development/Data/RawData/72316.txt'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>total_chars</th>\n",
       "      <th>total_words</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>word_max_length</th>\n",
       "      <th>word_min_length</th>\n",
       "      <th>mean_length</th>\n",
       "      <th>longest_word</th>\n",
       "      <th>shortest_word</th>\n",
       "      <th>sentence_max_length</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>longest_sentence</th>\n",
       "      <th>mtd_sentence</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>num_clauses</th>\n",
       "      <th>Haliday_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72316.txt</td>\n",
       "      <td>219078</td>\n",
       "      <td>50737.0</td>\n",
       "      <td>3374</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>4.317914</td>\n",
       "      <td>deputy-assistant-commissioner</td>\n",
       "      <td>a</td>\n",
       "      <td>131</td>\n",
       "      <td>21</td>\n",
       "      <td>Angel looked through the _dossier_\\ncarefully,...</td>\n",
       "      <td>It is unnecessary to introduce that world-famo...</td>\n",
       "      <td>13.230975</td>\n",
       "      <td>2641</td>\n",
       "      <td>254.184021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  total_chars  total_words  total_sentences  word_max_length  \\\n",
       "0  72316.txt       219078      50737.0             3374               29   \n",
       "\n",
       "   word_min_length  mean_length                   longest_word shortest_word  \\\n",
       "0                1     4.317914  deputy-assistant-commissioner             a   \n",
       "\n",
       "   sentence_max_length  max_tree_depth  \\\n",
       "0                  131              21   \n",
       "\n",
       "                                    longest_sentence  \\\n",
       "0  Angel looked through the _dossier_\\ncarefully,...   \n",
       "\n",
       "                                        mtd_sentence  lexical_density  \\\n",
       "0  It is unnecessary to introduce that world-famo...        13.230975   \n",
       "\n",
       "   num_clauses  Haliday_density  \n",
       "0         2641       254.184021  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Perform letter, word and n-tuple frequency analysis on text files.')\n",
    "    parser.add_argument('--filename', '-f', dest='inputfile', required=True, help='Text file to parse.')\n",
    "    parser.add_argument('--ntuple', '-n', dest='max_n_word', required=False, default=4, type=int, help='The maximum length n-tuple of words. Default is 4.')\n",
    "    parser.add_argument('--top', '-t', dest='top_n', required=False, default=20, type=int, help='List the top t most frequent n-words. Default is 20.')\n",
    "    parser.add_argument('--allow-digits', '-d', dest='allow_digits', default=False, required=False, help='Allow digits to be parsed (true/false). Default is false.')\n",
    "    parser.add_argument('--ignore', '-i', dest='ignore_list', required=False, help='Comma-delimted list of things to ignore')\n",
    " \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Update the inputfile argument with the desired file path\n",
    "    args.inputfile = 'C:/Source/Repos/Research-Proposal-development/Data/RawData/72316.txt'\n",
    "\n",
    "    w = wordsworth(args)\n",
    "    w.init_word_counters()\n",
    "    w.read_file()\n",
    "    w.compute_stats()  \n",
    "\n",
    "     \n",
    "    df = w.print_results()\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('output.csv')\n",
    "\n",
    "    # Or display it in a Jupyter notebook\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c5a830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file =  open('C:/Source/Repos/Research-Proposal-development/Data/RawData/72316.txt' , 'r', encoding='utf-8').read()       \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b3dd559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "longest_sentence = min(doc.sents, key=lambda sentence: len(sentence))\n",
    "print(longest_sentence.text)\n",
    "print(len(longest_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd8c5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_depth(sentence):\n",
    "    \"\"\"\n",
    "    Calculate the depth of the tree parsed by spaCy.\n",
    "    \"\"\"\n",
    "    def _depth(token):\n",
    "        if not list(token.children):\n",
    "            return 1\n",
    "        else:\n",
    "            return 1 + max(_depth(child) for child in token.children)\n",
    "    return max(_depth(token) for token in sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcc51f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum tree depth in the document: 21\n",
      "Sentence with max tree depth: It is unnecessary to introduce that world-famous publisher to\n",
      "the reader, the more particularly in view of the storm of controversy\n",
      "that burst about his robust figure in regard to the recent publication\n",
      "of Count Lehoff’s embarrassing “Memoirs.”\n",
      "Sentence: *** END OF THE PROJECT GUTENBERG EBOOK ANGEL ESQUIRE ***\n",
      "Tree depth: 6\n"
     ]
    }
   ],
   "source": [
    "max_depth = 0 \n",
    "sentence_mtd = ''\n",
    "for sent in doc.sents:\n",
    "    # Calculate the depth of the current sentence\n",
    "    depth = tree_depth(sent)\n",
    "    # Update the maximum depth if necessary\n",
    "    if depth > max_depth:\n",
    "        max_depth = depth\n",
    "        sentence_mtd = sent.text\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"Maximum tree depth in the document: {max_depth}\")\n",
    "print(f\"Sentence with max tree depth: {sentence_mtd}\")\n",
    "print(f\"Sentence: {sent.text}\")\n",
    "print(f\"Tree depth: {tree_depth(sent)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
