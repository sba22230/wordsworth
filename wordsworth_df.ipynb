{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b07101",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1502a8d",
   "metadata": {},
   "source": [
    "Name: wordsworth\n",
    "Description: Frequency analysis tool\n",
    "Author: autonomoid\n",
    "Date: 2014-06-22\n",
    "Licence: GPLv3\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program. If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2353dc70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f466f4e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class wordsworth:\n",
    "    args = 0\n",
    "    ignore_list = []\n",
    "    out = 0\n",
    "    words = []\n",
    "    previous_word = ''\n",
    "    previous_pair = ''\n",
    "    previous_triple = ''\n",
    "    previous_quad = ''\n",
    "    max_n_word = 4  \n",
    "    n_words = []\n",
    "    prev_n_words = []\n",
    "    counters = []\n",
    "\n",
    "    word_stats = {\n",
    "                  'file_name': '',\n",
    "                  'total_chars': 0,\n",
    "                  'total_words': 0,\n",
    "                  'max_length': 0,\n",
    "                  'min_length': 999,\n",
    "                  'mean_length': -1,\n",
    "                  'longest_word': '',\n",
    "                  'shortest_word': '',\n",
    "                #   'char_counts': {\n",
    "                #                   'a': 0.0, 'b': 0.0, 'c': 0.0, 'd': 0.0, 'e': 0.0, 'f': 0.0,\n",
    "                #                   'g': 0.0, 'h': 0.0, 'i': 0.0, 'j': 0.0, 'k': 0.0, 'l': 0.0,\n",
    "                #                   'm': 0.0, 'n': 0.0, 'o': 0.0, 'p': 0.0, 'q': 0.0, 'r': 0.0,\n",
    "                #                   's': 0.0, 't': 0.0, 'u': 0.0, 'v': 0.0, 'w': 0.0, 'x': 0.0,\n",
    "                #                   'y': 0.0, 'z': 0.0\n",
    "                #                  },\n",
    "                #   'char_percentages': {\n",
    "                #                        'a': 0.0, 'b': 0.0, 'c': 0.0, 'd': 0.0, 'e': 0.0, 'f': 0.0,\n",
    "                #                        'g': 0.0, 'h': 0.0, 'i': 0.0, 'j': 0.0, 'k': 0.0, 'l': 0.0,\n",
    "                #                        'm': 0.0, 'n': 0.0, 'o': 0.0, 'p': 0.0, 'q': 0.0, 'r': 0.0,\n",
    "                #                        's': 0.0, 't': 0.0, 'u': 0.0, 'v': 0.0, 'w': 0.0, 'x': 0.0,\n",
    "                #                        'y': 0.0, 'z': 0.0\n",
    "                #                       },\n",
    "                  'lexical_density': -1\n",
    "                  }\n",
    "\n",
    "\n",
    "    def __init__(self, commandline_args):\n",
    "        args = commandline_args\n",
    "        self.ignore_list = str(args.ignore_list).split(\",\")\n",
    "        \n",
    "\n",
    "    def print_results(self):\n",
    "    # Create a DataFrame from the word_stats dictionary\n",
    "        df = pd.DataFrame(self.word_stats, index=[0])\n",
    "        return df\n",
    "      \n",
    "        \n",
    "\n",
    "    def init_word_counters(self):\n",
    "        self.max_n_word = args.max_n_word\n",
    "        self.n_words = ['' for i in range(self.max_n_word)]\n",
    "        self.prev_n_words = ['' for i in range(self.max_n_word)]\n",
    "        self.counters = [collections.Counter() for i in range(self.max_n_word)]\n",
    "\n",
    "\n",
    "    def read_file(self):\n",
    "        print(\"[+] Analysing '\" + args.inputfile + \"'\")\n",
    "        self.word_stats['file_name'] = os.path.basename(args.inputfile)\n",
    "        if args.allow_digits:\n",
    "            self.words = re.findall(r\"['\\-\\w]+\", open(args.inputfile, 'r', encoding='utf-8').read().lower())\n",
    "        else:\n",
    "            self.words = re.findall(r\"['\\-A-Za-z]+\", open(args.inputfile, 'r', encoding='utf-8').read().lower())\n",
    "\n",
    "\n",
    "    def compute_stats(self):\n",
    "        for word in self.words:\n",
    "        \n",
    "            if word in self.ignore_list:\n",
    "                continue\n",
    "        \n",
    "            word = word.strip(r\"&^%$#@!\")\n",
    "\n",
    "            # Allow hyphenated words, but not hyphens as words on their own.\n",
    "            if word == '-':\n",
    "                continue\n",
    "\n",
    "            length = len(word)\n",
    "\n",
    "            # Record longest word length\n",
    "            if length > self.word_stats['max_length']:\n",
    "                self.word_stats['max_length'] = length\n",
    "                self.word_stats['longest_word'] = word\n",
    "\n",
    "            # Record shortest word length\n",
    "            if length < self.word_stats['min_length']:\n",
    "                self.word_stats['min_length'] = length\n",
    "                self.word_stats['shortest_word'] = word\n",
    "\n",
    "            # Keep track of the total number of words and chars read.\n",
    "            self.word_stats['total_chars'] += length\n",
    "            self.word_stats['total_words'] += 1.0\n",
    "\n",
    "            # # Note the charaters in each word.\n",
    "            # for char in word:\n",
    "            #     if char.lower() in self.word_stats['char_counts']:\n",
    "            #         self.word_stats['char_counts'][char.lower()] += 1.0\n",
    "\n",
    "            # Tally words.\n",
    "            for i in range(1, self.max_n_word):\n",
    "                if self.prev_n_words[i - 1] != '':\n",
    "                    self.n_words[i] = self.prev_n_words[i - 1] + ' ' + word\n",
    "                    self.counters[i][self.n_words[i]] += 1\n",
    "\n",
    "            self.n_words[0] = word\n",
    "            self.counters[0][word] += 1\n",
    "\n",
    "            for i in range(0, self.max_n_word):\n",
    "                self.prev_n_words[i] = self.n_words[i]\n",
    "\n",
    "        # Calculate the mean word length\n",
    "        self.word_stats['mean_length'] = self.word_stats['total_chars'] / self.word_stats['total_words']\n",
    "\n",
    "        # # Calculate relative character frequencies\n",
    "        # for char in self.word_stats['char_counts']:\n",
    "        #     char_count = self.word_stats['char_counts'][char]\n",
    "        #     total_chars = self.word_stats['total_chars']\n",
    "        #     percentage = 100.0 * (char_count / total_chars)\n",
    "        #     self.word_stats['char_percentages'][char] = percentage\n",
    "\n",
    "        # Calculate the lexical density of the text.\n",
    "        total_unique_words = len(self.counters[0])\n",
    "        total_words = sum(self.counters[0].values())\n",
    "        self.word_stats['lexical_density'] = 100.0 * total_unique_words / float(total_words)\n",
    "        return(self.word_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e221987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Analysing 'c:\\Users\\stehayes\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-9980Dm0UjEnd0cEB.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>file_name</th>\n",
       "      <td>kernel-v2-9980Dm0UjEnd0cEB.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chars</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_words</th>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_length</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_length</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_length</th>\n",
       "      <td>3.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longest_word</th>\n",
       "      <td>pythonjvsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shortest_word</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_density</th>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0\n",
       "file_name        kernel-v2-9980Dm0UjEnd0cEB.json\n",
       "total_chars                                  150\n",
       "total_words                                 44.0\n",
       "max_length                                    10\n",
       "min_length                                     1\n",
       "mean_length                             3.409091\n",
       "longest_word                          pythonjvsc\n",
       "shortest_word                                  a\n",
       "lexical_density                        72.727273"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Perform letter, word and n-tuple frequency analysis on text files.')\n",
    "    parser.add_argument('--filename', '-f', dest='inputfile', required=True, help='Text file to parse.')\n",
    "    parser.add_argument('--ntuple', '-n', dest='max_n_word', required=False, default=4, type=int, help='The maximum length n-tuple of words. Default is 4.')\n",
    "    parser.add_argument('--top', '-t', dest='top_n', required=False, default=20, type=int, help='List the top t most frequent n-words. Default is 20.')\n",
    "    parser.add_argument('--allow-digits', '-d', dest='allow_digits', default=False, required=False, help='Allow digits to be parsed (true/false). Default is false.')\n",
    "    parser.add_argument('--ignore', '-i', dest='ignore_list', required=False, help='Comma-delimted list of things to ignore')\n",
    " \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    w = wordsworth(args)\n",
    "    w.init_word_counters()\n",
    "    w.read_file()\n",
    "    w.compute_stats()  \n",
    "\n",
    "     \n",
    "    df = w.print_results()\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('output.csv')\n",
    "\n",
    "    # Or display it in a Jupyter notebook\n",
    "    display(df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
